{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6JT8xmP3ol8E",
    "outputId": "aca624b3-6107-4d2f-8d68-b4cc29f5175c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating gpt & smolvlm responses: 100%|██████████| 7371/7371 [1:46:14<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Scored CSV saved: /content/drive/MyDrive/mmml_project/outputs/gpt_vs_smolvlm_test_scored.csv\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "dataset_folder = \"/content/drive/MyDrive/mmml_project/outputs\"\n",
    "csv_files = {\n",
    "    \"gpt_vs_smolvlm\": os.path.join(dataset_folder, \"gpt_vs_smolvlm.csv\"),\n",
    "    \"llava_vs_blip\": os.path.join(dataset_folder, \"llava_vs_blip.csv\"),\n",
    "}\n",
    "\n",
    "client = OpenAI(api_key=\"OPENAI_KEY\")\n",
    "\n",
    "judge_prompt = \"\"\"You are an AI evaluator tasked with grading model responses. Your job is to assess if the given model response correctly answers the question, using both the answer and full answer for reference.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Full Answer: {full_answer}\n",
    "\n",
    "Model Response: {response}\n",
    "\n",
    "Instructions:\n",
    "- If the model response correctly aligns with the answer/full answer, return **1**.\n",
    "- If the model response is incorrect or misleading, return **0**.\n",
    "- **Only output a single digit (0 or 1), no explanations.**\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_responses(csv_path, strong_model, weak_model):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    strong_scores = []\n",
    "    weak_scores = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Evaluating {strong_model} & {weak_model} responses\"):\n",
    "        strong_prompt = judge_prompt.format(\n",
    "            question=row[\"question\"],\n",
    "            answer=row[\"answer\"],\n",
    "            full_answer=row[\"full_answer\"],\n",
    "            response=row[\"strong_model_response\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": strong_prompt}],\n",
    "            )\n",
    "            strong_score = response.choices[0].message.content.strip()\n",
    "            strong_scores.append(int(strong_score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing strong model row: {row['imageId']} - {e}\")\n",
    "            strong_scores.append(-1)\n",
    "\n",
    "        weak_prompt = judge_prompt.format(\n",
    "            question=row[\"question\"],\n",
    "            answer=row[\"answer\"],\n",
    "            full_answer=row[\"full_answer\"],\n",
    "            response=row[\"weak_model_response\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": weak_prompt}],\n",
    "            )\n",
    "            weak_score = response.choices[0].message.content.strip()\n",
    "            weak_scores.append(int(weak_score))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing weak model row: {row['imageId']} - {e}\")\n",
    "            weak_scores.append(-1)\n",
    "\n",
    "    df[f\"{strong_model}_score\"] = strong_scores\n",
    "    df[f\"{weak_model}_score\"] = weak_scores\n",
    "\n",
    "    output_csv = csv_path.replace(\".csv\", \"_test_scored.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Test Scored CSV saved: {output_csv}\")\n",
    "\n",
    "evaluate_responses(csv_files[\"gpt_vs_smolvlm\"], \"gpt\", \"smolvlm\")\n",
    "#evaluate_responses(csv_files[\"llava_vs_blip\"], \"llava\", \"blip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuEk1gDYp58v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
